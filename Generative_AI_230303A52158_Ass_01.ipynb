{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNt/t/fe8mzTyig8tYOogQ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52158/GENERATIVE-AI_2025/blob/main/Generative_AI_230303A52158_Ass_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. (1 ponto) Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        "values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        "with the outcomes of libraries**\n",
        "\n",
        "         Y_Actual               Y_Pred\n",
        "\n",
        "           20                     20.5        \n",
        "           30                     30.3\n",
        "           40                     40.2\n",
        "           50                     50.6\n",
        "           60                     60.7\n",
        "\n",
        "          Tabela 1: YActual Vs. YP red"
      ],
      "metadata": {
        "id": "-5KnYfCTkBd3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gRhUnK2-eqIz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_actual = np.array([20, 30, 40, 50, 60])\n",
        "y_pred = np.array([20.5, 30.3, 40.2, 50.6, 60.7])"
      ],
      "metadata": {
        "id": "syMmvLsWnRTb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae = mean_absolute_error(y_actual, y_pred)\n",
        "mse = mean_squared_error(y_actual, y_pred)\n",
        "rmse = sqrt(mse)\n",
        "r2 = r2_score(y_actual, y_pred)"
      ],
      "metadata": {
        "id": "nNhiVOOEnUVQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Regression Metrics:\")\n",
        "print(f\"Mean Absolute Error (MAE) {mae}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R-squared (R²): {r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piG6Bpd0nyE1",
        "outputId": "41290f03-24ce-4f5d-c70e-9cf2d05df62e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression Metrics:\n",
            "Mean Absolute Error (MAE) 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "R-squared (R²): 0.99877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "CORTSnCNp4F3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_sklearn = mean_absolute_error(y_actual, y_pred)\n",
        "mse_sklearn = mean_squared_error(y_actual, y_pred)\n",
        "r2_sklearn = r2_score(y_actual, y_pred)"
      ],
      "metadata": {
        "id": "_rdBpb2OqoXB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nlibraries(scikit-learn):\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_sklearn:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_sklearn:.2f}\")\n",
        "print(f\"R-squared (R²): {r2_sklearn:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2TK4HNlotzB",
        "outputId": "fdc721a3-7a59-440f-9a1a-ddd8d3a9be9c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "libraries(scikit-learn):\n",
            "Mean Absolute Error (MAE): 0.46\n",
            "Mean Squared Error (MSE): 0.25\n",
            "R-squared (R²): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. (1 ponto) Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries**\n",
        "\n",
        "          Y_Actual              Y_Pred\n",
        "            0                     0      1  1  2  0\n",
        "            0                     0      1  0  2  0\n",
        "            0                     1      1  2  2  1\n",
        "            0                     2      1  0  2  2\n",
        "            0                     2      1  2  2  2\n",
        "              Tabela 2: YActual Vs. YP red"
      ],
      "metadata": {
        "id": "VJ-3cAybwQHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "v4n2aEAywOpa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_actual = np.array([0, 0, 0, 0, 0])\n",
        "y_pred = np.array([\n",
        "    [0, 1, 1, 2, 0],\n",
        "    [0, 1, 0, 2, 0],\n",
        "    [1, 1, 2, 2, 1],\n",
        "    [0, 2, 1, 0, 2],\n",
        "    [2, 1, 2, 2, 2],\n",
        "])"
      ],
      "metadata": {
        "id": "n9hDolF0yQLg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_actual = np.repeat(y_actual, y_pred.shape[1])\n",
        "y_pred = y_pred.flatten()"
      ],
      "metadata": {
        "id": "GRD0B4b-6gGp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_actual, y_pred)\n",
        "accuracy = accuracy_score(y_actual, y_pred)\n",
        "precision = precision_score(y_actual, y_pred, average='macro', zero_division=0)\n",
        "recall = recall_score(y_actual, y_pred, average='macro', zero_division=0)\n",
        "f1 = f1_score(y_actual, y_pred, average='macro', zero_division=0)"
      ],
      "metadata": {
        "id": "a2iS83ZA6pk8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYQrVSQT7IJH",
        "outputId": "3a4e6463-4667-447c-c428-36b37dd9d246"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[ 7  8 10]\n",
            " [ 0  0  0]\n",
            " [ 0  0  0]]\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.28\n",
            "Precision: 0.3333333333333333\n",
            "Recall: 0.09333333333333334\n",
            "F1-Score: 0.14583333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "Y_actual = np.array([0, 0, 1, 1, 2, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 2])\n",
        "Y_pred = np.array([0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 2, 0, 2, 1, 2, 2, 2])"
      ],
      "metadata": {
        "id": "-e4nFmnP7xmE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(Y_actual)\n",
        "\n",
        "true_positive = {0: 0, 1: 0, 2: 0}\n",
        "false_positive = {0: 0, 1: 0, 2: 0}\n",
        "false_negative = {0: 0, 1: 0, 2: 0}\n",
        "true_negative = {0: 0, 1: 0, 2: 0}"
      ],
      "metadata": {
        "id": "bBswszV077Ts"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n):\n",
        "    for cls in [0, 1, 2]:\n",
        "        if Y_actual[i] == cls and Y_pred[i] == cls:\n",
        "            true_positive[cls] += 1\n",
        "        elif Y_actual[i] != cls and Y_pred[i] == cls:\n",
        "            false_positive[cls] += 1\n",
        "        elif Y_actual[i] == cls and Y_pred[i] != cls:\n",
        "            false_negative[cls] += 1\n",
        "        elif Y_actual[i] != cls and Y_pred[i] != cls:\n",
        "            true_negative[cls] += 1"
      ],
      "metadata": {
        "id": "Cp21ubGo8WIg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_scratch = sum([true_positive[cls] for cls in [0, 1, 2]]) / n\n",
        "precision_scratch = {cls: true_positive[cls] / (true_positive[cls] + false_positive[cls]) if (true_positive[cls] + false_positive[cls]) > 0 else 0 for cls in [0, 1, 2]}\n",
        "recall_scratch = {cls: true_positive[cls] / (true_positive[cls] + false_negative[cls]) if (true_positive[cls] + false_negative[cls]) > 0 else 0 for cls in [0, 1, 2]}\n",
        "f1_score_scratch = {cls: 2 * precision_scratch[cls] * recall_scratch[cls] / (precision_scratch[cls] + recall_scratch[cls]) if (precision_scratch[cls] + recall_scratch[cls]) > 0 else 0 for cls in [0, 1, 2]}"
      ],
      "metadata": {
        "id": "G6ILBqTA-J__"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Metrics calculated from scratch:\")\n",
        "print(f\"Accuracy: {accuracy_scratch}\")\n",
        "for cls in [0, 1, 2]:\n",
        "    print(f\"Class {cls} - Precision: {precision_scratch[cls]}, Recall: {recall_scratch[cls]}, F1-Score: {f1_score_scratch[cls]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klkfX-68-SJ9",
        "outputId": "c2fb6edd-9464-42f3-e3c4-ba1eeccab407"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics calculated from scratch:\n",
            "Accuracy: 0.7333333333333333\n",
            "Class 0 - Precision: 0.7777777777777778, Recall: 0.6363636363636364, F1-Score: 0.7000000000000001\n",
            "Class 1 - Precision: 0.7142857142857143, Recall: 0.625, F1-Score: 0.6666666666666666\n",
            "Class 2 - Precision: 0.7142857142857143, Recall: 0.9090909090909091, F1-Score: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_library = accuracy_score(Y_actual, Y_pred)\n",
        "precision_library = precision_score(Y_actual, Y_pred, average=None)\n",
        "recall_library = recall_score(Y_actual, Y_pred, average=None)\n",
        "f1_library = f1_score(Y_actual, Y_pred, average=None)"
      ],
      "metadata": {
        "id": "8ydGRLAq-sUP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMetrics calculated using libraries:\")\n",
        "print(f\"Accuracy: {accuracy_library}\")\n",
        "for cls, (p, r, f1) in enumerate(zip(precision_library, recall_library, f1_library)):\n",
        "    print(f\"Class {cls} - Precision: {p}, Recall: {r}, F1-Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6PRkzJC-tef",
        "outputId": "6ea0ae30-5710-4800-da1b-61f71f2ffa10"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics calculated using libraries:\n",
            "Accuracy: 0.7333333333333333\n",
            "Class 0 - Precision: 0.7777777777777778, Recall: 0.6363636363636364, F1-Score: 0.7\n",
            "Class 1 - Precision: 0.7142857142857143, Recall: 0.625, F1-Score: 0.6666666666666666\n",
            "Class 2 - Precision: 0.7142857142857143, Recall: 0.9090909090909091, F1-Score: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(Y_actual, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY7vwQd2-x57",
        "outputId": "74eeaf8c-5df1-4899-a48f-4041c866fd34"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.64      0.70        11\n",
            "           1       0.71      0.62      0.67         8\n",
            "           2       0.71      0.91      0.80        11\n",
            "\n",
            "    accuracy                           0.73        30\n",
            "   macro avg       0.74      0.72      0.72        30\n",
            "weighted avg       0.74      0.73      0.73        30\n",
            "\n"
          ]
        }
      ]
    }
  ]
}